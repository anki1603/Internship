{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75baf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ankit\\anaconda3\\lib\\site-packages (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ankit\\anaconda3\\lib\\site-packages (from requests) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38131d3a",
   "metadata": {},
   "source": [
    "# 1)\tWrite a python program to display all the header tags from wikipedia.org and make data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56af07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220d884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d27d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19c22b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://en.wikipedia.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fe7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ae100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "433c433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tags = soup.find_all(['h1','h2','h3','h4','h5','h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa8ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tag_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e73f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in header_tags:\n",
    "    header_tag_data.append({\n",
    "        'tag': tag.name,\n",
    "        'text': tag.text.strip()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "672039c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(header_tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "661fb47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tag                           text\n",
      "0  h1                      Main Page\n",
      "1  h1           Welcome to Wikipedia\n",
      "2  h2  From today's featured article\n",
      "3  h2               Did you know ...\n",
      "4  h2                    In the news\n",
      "5  h2                    On this day\n",
      "6  h2       Today's featured picture\n",
      "7  h2       Other areas of Wikipedia\n",
      "8  h2    Wikipedia's sister projects\n",
      "9  h2            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323f6aa",
   "metadata": {},
   "source": [
    "# 2)\tWrite s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3423da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1126caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3b2c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36e7c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32d24749",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02c229d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73063fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in soup.find_all('table'):\n",
    "    rows = table.find_all('tr')  # Correct indentation here\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        term_of_office = columns[1].text.strip()\n",
    "        # Append data to the list\n",
    "        presidents.append({'Name': name, 'Term of office': term_of_office})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6dac1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3697ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe98b3",
   "metadata": {},
   "source": [
    "# Question 3= Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5542eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_top_10_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/men/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    teams = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        rating = columns[2].text\n",
    "        matches = columns[3].text\n",
    "        points = columns[4].text\n",
    "\n",
    "        teams.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'rating': rating,\n",
    "            'matches': matches,\n",
    "            'points': points\n",
    "        })\n",
    "\n",
    "    return teams[:10]\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "for team in get_top_10_teams():\n",
    "    print(team)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3597c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_top_10_batsman():\n",
    "    url = \"https://www.icc-cricket.com/rankings/men/batting/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batsmen = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        team = columns[2].find('a').text\n",
    "        rating = columns[3].text\n",
    "\n",
    "        batsmen.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'team': team,\n",
    "            'rating': rating\n",
    "        })\n",
    "\n",
    "    return batsmen[:10]\n",
    "\n",
    "print(\"Top 10 ODI Batsmen:\")\n",
    "for batsman in get_top_10_batsmen():\n",
    "    print(batsman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de231def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Bowlers:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_top_10_bowlers():\n",
    "    url = \"https://www.icc-cricket.com/rankings/men/bowling/odi\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    bowlers = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        team = columns[2].find('a').text\n",
    "        rating = columns[3].text\n",
    "\n",
    "        bowlers.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'team': team,\n",
    "            'rating': rating\n",
    "        })\n",
    "\n",
    "    return bowlers[:10]\n",
    "\n",
    "print(\"Top 10 ODI Bowlers:\")\n",
    "for bowler in get_top_10_bowlers():\n",
    "    print(bowler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c84915",
   "metadata": {},
   "source": [
    "# question4)\tWrite a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame- \n",
    "a)\tTop 10 ODI teams in women’s cricket along with the records for matches, points and rating. \n",
    "b)\tTop 10 women’s ODI Batting players along with the records of their team and rating. \n",
    "c)\tTop 10 women’s ODI all-rounder along with the records of their team and rating. \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59a62bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_10_teams():\n",
    "    url = \"https://www.icc-cricket.com/womens-cricket/rankings/odi-team-rankings\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    teams = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        rating = columns[2].text\n",
    "        matches = columns[3].text\n",
    "        points = columns[4].text\n",
    "\n",
    "        teams.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'rating': rating,\n",
    "            'matches': matches,\n",
    "            'points': points\n",
    "        })\n",
    "\n",
    "    return teams[:10]\n",
    "\n",
    "team_data = get_top_10_teams()\n",
    "team_df = pd.DataFrame(team_data)\n",
    "print(team_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f6f6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_10_batsmen():\n",
    "    url = \"https://www.icc-cricket.com/womens-cricket/rankings/odi-batting\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batsmen = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        team = columns[2].find('a').text\n",
    "        rating = columns[3].text\n",
    "\n",
    "        batsmen.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'team': team,\n",
    "            'rating': rating\n",
    "        })\n",
    "\n",
    "    return batsmen[:10]\n",
    "\n",
    "batsmen_data = get_top_10_batsmen()\n",
    "batsmen_df = pd.DataFrame(batsmen_data)\n",
    "print(batsmen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d78dd772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_10_allrounders():\n",
    "    url = \"https://www.icc-cricket.com/womens-cricket/rankings/odi-all-rounder\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    allrounders = []\n",
    "    for row in soup.find_all('tr', {'class': 'template-item'}):\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text\n",
    "        name = columns[1].find('a').text\n",
    "        team = columns[2].find('a').text\n",
    "        rating = columns[3].text\n",
    "\n",
    "        allrounders.append({\n",
    "            'rank': rank,\n",
    "            'name': name,\n",
    "            'team': team,\n",
    "            'rating': rating\n",
    "        })\n",
    "\n",
    "    return allrounders[:10]\n",
    "\n",
    "allrounders_data = get_top_10_allrounders()\n",
    "allrounders_df = pd.DataFrame(allrounders_data)\n",
    "print(allrounders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647821b",
   "metadata": {},
   "source": [
    "# 5)\tWrite a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame- i) Headline ii) Time \n",
    "iii) News Link \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72d9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e080ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = soup.find_all('article', class_='LatestNews__articleContainer--3FtBh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1459bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "times = []\n",
    "links = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4315616",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in news_articles:\n",
    "    headline = article.find('h3', class_='LatestNews__headline--2YGnS').text.strip()\n",
    "    time = article.find('time', class_='LatestNews__timestamp--2NMxZ').text.strip()\n",
    "    link = 'https://www.cnbc.com' + article.find('a')['href']\n",
    "\n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce350952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Headline': headlines,\n",
    "    'Time': times,\n",
    "    'News Link': links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8dc660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f40eb",
   "metadata": {},
   "source": [
    "# 6)\tWrite a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame- i) \tPaper Title ii) \tAuthors iii) \tPublished Date iv) \tPaper URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc6028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c617c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb4a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf8dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_details(article):\n",
    "    title = article.h3.text.strip()\n",
    "    authors = article.find(\"span\", {\"class\": \"Authors\"}).text.strip()\n",
    "    published_date = article.find(\"span\", {\"class\": \"pubDate\"}).text.strip()\n",
    "    paper_url = article.find(\"a\", {\"class\": \"title\"})[\"href\"]\n",
    "    return title, authors, published_date, paper_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4927bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c42c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e982fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all(\"div\", {\"class\": \"articleItem\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8657f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_details = [get_article_details(article) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e10b2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(article_details, columns=[\"Title\", \"Authors\", \"Published Date\", \"Paper URL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23f50d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3edf0",
   "metadata": {},
   "source": [
    "# 7)\tWrite a python program to scrape mentioned details from dineout.co.in and make data frame- \n",
    "i)\tRestaurant name \n",
    "ii)\tCuisine \n",
    "iii) Location \n",
    "iv) \tRatings \n",
    " v) \tImage URL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc063e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the elements containing the restaurant details\n",
    "restaurant_elements = soup.find_all(\"div\", class_=\"restaurant-details\")\n",
    "\n",
    "# Initialize empty lists to store the details\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Loop through each restaurant element and extract the details\n",
    "for element in restaurant_elements:\n",
    "    # Extract the restaurant name\n",
    "    restaurant_name = element.find(\"h2\").text.strip()\n",
    "    restaurant_names.append(restaurant_name)\n",
    "    \n",
    "    # Extract the cuisine\n",
    "    cuisine = element.find(\"span\", class_=\"cuisine\").text.strip()\n",
    "    cuisines.append(cuisine)\n",
    "    \n",
    "    # Extract the location\n",
    "    location = element.find(\"span\", class_=\"location\").text.strip()\n",
    "    locations.append(location)\n",
    "    \n",
    "    # Extract the ratings\n",
    "    rating = element.find(\"span\", class_=\"rating\").text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Extract the image URL\n",
    "    image_url = element.find(\"img\")[\"src\"]\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a data frame using the extracted details\n",
    "data = {\n",
    "    \"Restaurant Name\": restaurant_names,\n",
    "    \"Cuisine\": cuisines,\n",
    "    \"Location\": locations,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Image URL\": image_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464ae5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
